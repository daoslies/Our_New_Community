{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IpRDUOYvJX21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1baf7f6-2cb5-4174-850b-3062b94f587f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Reward function in conversation will be made up of three components:\n",
        "- r_c = congruence reward: how likely is the agent to have said what the respondent said (negative KL divergence of the next token probabilities)\n",
        "- r_s = sentiment reward: how positive was the sentiment of the respondent (use a pre-existing sentiment model)\n",
        "- r_a = affection reward: how much does the agent like the respondent (use discounted sum of previous rewards)\n",
        "\n",
        "r=(r_c+r_s)*r_a + epsilon, where epsilon is some very small noise term\n",
        "r_c and r_s should be distributed around zero, i.e. can be positive or negative\n",
        "r_a should be between 0 and 1\n",
        "\n",
        "After this, the plan is to build framework for models talking to each other, where you can import the models you want and\n",
        "the reward functions you want and then to the RLHF conversation loop\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "def get_congruence_reward(comment_ids, response_ids_trunc, agent_model):\n",
        "    \"\"\"Iterate through respondent's response to agent's comment, adding each token to the\n",
        "    prompt each time, and get the probability for what the agent would have said instead.\n",
        "    Take the ratio of this with their max probability for saying anything and average.\n",
        "\n",
        "    Args:\n",
        "    comment_ids (torch tensor): IDs of agent's comment, including original query.\n",
        "    response_ids_trunc (torch tensor): IDs of respondent's response, not including\n",
        "        original comment or query\n",
        "    agent_model (transformers GPT2LMHeadModel): the agent model\n",
        "\n",
        "    Returns:\n",
        "    float: Congruence reward value\n",
        "    \"\"\"\n",
        "\n",
        "    prob_ratios = list()\n",
        "    target_ids = comment_ids.clone()\n",
        "\n",
        "    for id in response_ids_trunc[0]:\n",
        "        agent_output = agent(target_ids)\n",
        "        agent_probs = F.softmax(agent_output.logits[0][-1], dim=0) # Predicted probs\n",
        "        id_prob = agent_probs[id].item() # The negative of the log of this is the KL divergence\n",
        "        max_prob = agent_probs.max().item()\n",
        "        prob_ratio = id_prob / max_prob\n",
        "        prob_ratios.append(prob_ratio)\n",
        "        target_ids = torch.cat((target_ids.squeeze(0), id.unsqueeze(0)), dim=0).unsqueeze(0)\n",
        "\n",
        "    congruence_reward = 2*np.mean(prob_ratios) - 1\n",
        "\n",
        "    return congruence_reward\n",
        "\n",
        "\n",
        "def get_sentiment_reward(response_text_trunc, sentiment_tokenizer, sentiment_model):\n",
        "    \"\"\"Get a scalar reward corresponding to sentiment of respondent's response.\n",
        "\n",
        "    Args:\n",
        "    response_text_trunc (torch tensor): text of respondent's response, not including\n",
        "        original comment or query\n",
        "    sentiment_tokenizer (transformers AutoTokenizer): tokenizer for sentiment model\n",
        "    sentiment_tokenizer (transformers AutoModelForSequenceClassification):\n",
        "        sentiment model\n",
        "\n",
        "    Returns:\n",
        "    float: Sentiment reward value\n",
        "    \"\"\"\n",
        "\n",
        "    # Get sentiment probabilities from model (negative, neutral, or positive)\n",
        "    sentiment_response_ids = sentiment_tokenizer.encode(response_text_trunc, return_tensors=\"pt\").to('cuda')\n",
        "    sentiment_probs = F.softmax(sentiment_model(sentiment_response_ids).logits.detach(), dim=1)[0]\n",
        "\n",
        "    # Calculate the reward as the positive probability minus the negative probability\n",
        "    sentiment_reward = (sentiment_probs[2] - sentiment_probs[0]).item()\n",
        "\n",
        "    return sentiment_reward\n",
        "\n",
        "\n",
        "def get_affection_reward(affection_counter):\n",
        "    \"\"\"Given just by affection_counter.\"\"\"\n",
        "    affection_reward = affection_counter\n",
        "    return affection_reward\n",
        "\n",
        "\n",
        "def update_affection_counter(affection_counter, last_reward, damping=0.5):\n",
        "    \"\"\"Update scalar affection counter with recent reward.\n",
        "\n",
        "    Args:\n",
        "        affection_counter (float): Long-term value for liking other agent\n",
        "        last_reward (float): Last value of the combined reward\n",
        "        damping (float): Fraction to slow updates by\n",
        "\n",
        "    Returns:\n",
        "        float: Updated long-term value for liking other agent\n",
        "    \"\"\"\n",
        "\n",
        "    affection_counter = affection_counter*(1 + damping*last_reward)\n",
        "    return affection_counter\n",
        "\n",
        "\n",
        "def reward_combiner(congruence_reward, sentiment_reward, affection_reward, epsilon=0.01):\n",
        "    \"\"\"Combine all three reward terms and add noise.\n",
        "\n",
        "    Args:\n",
        "        congruence_reward (float): Congruence reward value\n",
        "        sentiment_reward (float): Sentiment reward value\n",
        "        affection_reward (float): Affection reward value\n",
        "        epsilon (float): Scaling factor to apply to exponentially distributed noise term\n",
        "\n",
        "    Returns:\n",
        "        float: Combined reward\n",
        "    \"\"\"\n",
        "\n",
        "    noise_term = epsilon*np.random.exponential(1)\n",
        "    combined_reward = (congruence_reward + sentiment_reward)*affection_reward + noise_term\n",
        "    return combined_reward\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_path = './drive/MyDrive/Colab Notebooks/GPT_community/'"
      ],
      "metadata": {
        "id": "hsUMhTbN6cgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7804add-a1f2-4ebb-871b-cb56cb97058a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in prompts\n",
        "prompts_file = os.path.join(project_path, 'data/brighton_philosophy_prompts.txt')\n",
        "with open(prompts_file) as file:\n",
        "    prompts = [line.rstrip() for line in file]\n",
        "\n",
        "# Create agent and respondent models\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "agent = GPT2LMHeadModel.from_pretrained('Linus4Lyf/Kant_Metaphysics_Of_Morals').to('cuda')\n",
        "respondent = GPT2LMHeadModel.from_pretrained('Linus4Lyf/Hume_A_Treatise_Of_Human_Nature').to('cuda')\n",
        "\n",
        "# Create sentiment model\n",
        "sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "sent_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to('cuda')\n",
        "\n",
        "# Initialise affection counter at neutral value\n",
        "affection_counter = 0.5"
      ],
      "metadata": {
        "id": "xmjEjtdZ3EjW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questioner_name = 'Socrates'\n",
        "agent_name = 'Kant'\n",
        "respondent_name = 'Hume'\n",
        "\n",
        "# Get query from questions list\n",
        "query_text = f\"{questioner_name}: \" + np.random.choice(prompts)\n",
        "print(query_text, '\\n')\n",
        "query_text += f\"\\n{agent_name}: \"\n",
        "\n",
        "# Encode query and get comment from agent\n",
        "query_ids = tokenizer.encode(query_text, return_tensors='pt').to('cuda')\n",
        "comment_ids = agent.generate(query_ids, do_sample=True, temperature=0.9, max_new_tokens=200, pad_token_id=pad_token_id, eos_token_id=pad_token_id)\n",
        "comment_text = tokenizer.batch_decode(comment_ids)[0]\n",
        "print('--------------------------------------------------------------------------------------')\n",
        "print(comment_text, '\\n')\n",
        "comment_text += f\"\\n{respondent_name}: \"\n",
        "\n",
        "# Get response from respondent\n",
        "comment_ids = tokenizer.encode(comment_text, return_tensors='pt').to('cuda')\n",
        "response_ids = respondent.generate(comment_ids, do_sample=True, temperature=0.9, max_new_tokens=200, pad_token_id=pad_token_id, eos_token_id=pad_token_id)\n",
        "response_text = tokenizer.batch_decode(response_ids)[0]\n",
        "print('--------------------------------------------------------------------------------------')\n",
        "print(response_text, '\\n')\n",
        "\n",
        "# Remove original query and comment from response text\n",
        "response_text_trunc = response_text.replace(comment_text, '')[1:]\n",
        "response_ids_trunc = tokenizer.encode(response_text_trunc, return_tensors='pt').to('cuda')\n",
        "print('--------------------------------------------------------------------------------------')\n",
        "print(response_text_trunc, '\\n')\n",
        "\n",
        "# Get reward for response\n",
        "congruence_reward = get_congruence_reward(comment_ids, response_ids_trunc, agent)\n",
        "sentiment_reward = get_sentiment_reward(response_text_trunc, sent_tokenizer, sent_model)\n",
        "affection_reward = get_affection_reward(affection_counter)\n",
        "print(f\"Congruence reward = {congruence_reward}, Sentiment reward = {sentiment_reward}, Affection reward = {affection_reward}\")\n",
        "\n",
        "# Combine components of reward\n",
        "combined_reward = reward_combiner(congruence_reward, sentiment_reward, affection_reward, epsilon=0.01)\n",
        "print(f\"Combined reward = {combined_reward}\")\n",
        "\n",
        "# Update affection counter\n",
        "affection_counter = update_affection_counter(affection_counter, combined_reward)\n",
        "print(f\"New affection counter = {affection_counter}\")"
      ],
      "metadata": {
        "id": "tujTiucHSmTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a11de0-bd79-4954-c737-a422b22cbbfd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Socrates: Is religion a force for good? \n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            "Socrates: Is religion a force for good?\n",
            "Kant:  “It might be said that,” according to his teaching, ’man is the supreme and perfect agent of nature”\n",
            "Or in other words: “The human will ’is the result of the common interest of all the creatures, and the supreme and perfect agent of their nature.\n",
            "Does that imply a general principle (of moral good)? Is there any such thing as a universal good? Not in my case, because this might be considered a kind of universal law only for the most part because it presupposes a universal law for each, but because its presupposition will only provide us with a general principle for the very same reasons.\n",
            "\n",
            "[removed]\n",
            "Or, as Kant calls it, a certain general law; and if we conceive of it as a universal law for every kind of creature, and the universal law as being the universal law that all animals, which cannot be satisfied with this universal law, cannot be persuaded of, we should be able \n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            "Socrates: Is religion a force for good?\n",
            "Kant:  “It might be said that,” according to his teaching, ’man is the supreme and perfect agent of nature”\n",
            "Or in other words: “The human will ’is the result of the common interest of all the creatures, and the supreme and perfect agent of their nature.\n",
            "Does that imply a general principle (of moral good)? Is there any such thing as a universal good? Not in my case, because this might be considered a kind of universal law only for the most part because it presupposes a universal law for each, but because its presupposition will only provide us with a general principle for the very same reasons.\n",
            "\n",
            "[removed]\n",
            "Or, as Kant calls it, a certain general law; and if we conceive of it as a universal law for every kind of creature, and the universal law as being the universal law that all animals, which cannot be satisfied with this universal law, cannot be persuaded of, we should be able\n",
            "Hume:  Now it is plain, that, if the general principle be unknown, and reason, that every thing is a universal good, can be so perfectly unknown, as to be entirely impossible, it is impossible we can comprehend it. For if the general principle be unknown, it will never admit of a force, for reason, which cannot be independent of cause. For we cannot comprehend the universal law, why the common interest of every creature, without any other causes, can be so perfectly unknown and unknown, as to be utterly impossible. But though that be not certain, why some creatures are endowed with certain powers, which make them enter into a certain arrangement; it is evident, that every creature is endowed with certain powers, which give them a certain inclination to this particular arrangement. We would not say, That this inclination is derived from any other power, but that it is the entire and universal property of all such creatures. For it is impossible, that any creature is endowed with any power of any kind \n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            "Now it is plain, that, if the general principle be unknown, and reason, that every thing is a universal good, can be so perfectly unknown, as to be entirely impossible, it is impossible we can comprehend it. For if the general principle be unknown, it will never admit of a force, for reason, which cannot be independent of cause. For we cannot comprehend the universal law, why the common interest of every creature, without any other causes, can be so perfectly unknown and unknown, as to be utterly impossible. But though that be not certain, why some creatures are endowed with certain powers, which make them enter into a certain arrangement; it is evident, that every creature is endowed with certain powers, which give them a certain inclination to this particular arrangement. We would not say, That this inclination is derived from any other power, but that it is the entire and universal property of all such creatures. For it is impossible, that any creature is endowed with any power of any kind \n",
            "\n",
            "Congruence reward = 0.17535917865804573, Sentiment reward = -0.22970479726791382, Affection reward = 0.5130949537183446\n",
            "Combined reward = -0.025448613712709022\n",
            "New affection counter = 0.5065661760807854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ey-wd6qZBF-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}