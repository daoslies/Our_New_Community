{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3s9+olnhtyTjWBOijaMB2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"IpRDUOYvJX21","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71512844-7b7d-494e-855f-7384f91777b3","executionInfo":{"status":"ok","timestamp":1683294485693,"user_tz":-60,"elapsed":21540,"user":{"displayName":"Richard Juggins","userId":"16618076254761154760"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["\"\"\"Reward function in conversation will be made up of three components:\n","- r_c = congruence reward: how likely is the agent to have said what the respondent said (negative KL divergence of the next token probabilities)\n","- r_s = sentiment reward: how positive was the sentiment of the respondent (use a pre-existing sentiment model)\n","- r_a = affection reward: how much does the agent like the respondent (use discounted sum of previous rewards)\n","\n","r=r_c*r_s*r_a + epsilon, where epsilon is some very small noise term\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from google.colab import drive\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","def get_congruence_reward(comment_ids, response_ids_trunc, agent_model):\n","    \"\"\"Iterate through respondent's response to agent's comment, adding each token to the\n","    prompt each time, and get the KL divergence for what the agent would have said instead.\n","    Take the mean of all KL divergences at the end to give congruence reward.\n","\n","    Args:\n","    comment_ids (torch tensor): IDs of agent's comment, including original query.\n","    response_ids_trunc (torch tensor): IDs of respondent's response, not including\n","        original comment or query\n","    agent_model (transformers GPT2LMHeadModel): the agent model\n","\n","    Returns:\n","    float: Congruence reward value\n","    \"\"\"\n","\n","    KL_divs = list()\n","    target_ids = comment_ids.clone()\n","\n","    for id in response_ids_trunc[0]:\n","        agent_output = agent(target_ids)\n","        agent_log_probs = F.log_softmax(agent_output.logits[0][-1], dim=0) # Predicted log probs\n","        KL_div = -agent_log_probs[id] # Calculate KL divergence (same as cross-entropy in this case)\n","        KL_divs.append(KL_div.item())\n","        target_ids = torch.cat((target_ids.squeeze(0), id.unsqueeze(0)), dim=0).unsqueeze(0)\n","\n","    mean_div = np.mean(KL_divs)\n","    congruence_reward = np.sqrt(np.exp(-mean_div))\n","\n","    return congruence_reward\n","\n","\n","def get_sentiment_reward(response_text_trunc, sentiment_tokenizer, sentiment_model):\n","    \"\"\"Get a scalar reward corresponding to sentiment of respondent's response.\n","\n","    Args:\n","    response_text_trunc (torch tensor): text of respondent's response, not including\n","        original comment or query\n","    sentiment_tokenizer (transformers AutoTokenizer): tokenizer for sentiment model\n","    sentiment_tokenizer (transformers AutoModelForSequenceClassification):\n","        sentiment model\n","\n","    Returns:\n","    float: Sentiment reward value\n","    \"\"\"\n","\n","    # Get sentiment probabilities from model (negative, neutral, or positive)\n","    sentiment_response_ids = sentiment_tokenizer.encode(response_text_trunc, return_tensors=\"pt\").to('cuda')\n","    sentiment_probs = F.softmax(sentiment_model(sentiment_response_ids).logits.detach(), dim=1)[0]\n","\n","    # Calculate the reward as the positive probability minus the negative probability\n","    sentiment_reward = (sentiment_probs[2] - sentiment_probs[0]).item()\n","\n","    # Scale to be between 0 and 1\n","    sentiment_reward = (sentiment_reward + 1) / 2\n","\n","    return sentiment_reward\n","\n","\n","def get_affection_reward(affection_counter):\n","    \"\"\"Given by cube root of affection_counter.\"\"\"\n","    affection_reward = np.cbrt(affection_counter)\n","    return affection_reward\n","\n","\n","def update_affection_counter(affection_counter, last_raw_reward, discount=0.1):\n","    \"\"\"Update scalar affection counter with discounted most recent raw\n","    reward, i.e. reward before scaling away from between 0 and 1.\n","\n","    Args:\n","        affection_counter (float): Long-term value for liking other agent\n","        last_raw_reward (float): Number between 0 and 1 from last unprocessed\n","            reward\n","        discount (float): Fraction deciding how fast to update counter\n","\n","    Returns:\n","        float: Updated long-term value for liking other agent\n","    \"\"\"\n","\n","    affection_counter = (1-discount)*affection_counter + discount*last_raw_reward\n","    return affection_counter\n","\n","\n","def raw_reward_combiner(congruence_reward, sentiment_reward, affection_reward, epsilon=0.01):\n","    \"\"\"Combine all three reward terms and add noise.\n","\n","    Args:\n","        congruence_reward (float): Congruence reward value\n","        sentiment_reward (float): Sentiment reward value\n","        affection_reward (float): Affection reward value\n","        epsilon (float): Scaling factor to apply to exponentially distributed noise term\n","\n","    Returns:\n","        float: Combined raw reward between 0 and 1\n","    \"\"\"\n","\n","    noise_term = epsilon*np.random.exponential(1)\n","    combined_raw_reward = congruence_reward*sentiment_reward*affection_reward + noise_term\n","    return combined_raw_reward\n","\n","\n","def scale_combined_reward(combined_raw_reward, amplitude=10):\n","    \"\"\"Scale reward values by some method to make them work better.\n","    For now will just multiply by a constant factor.\n","\n","    Args:\n","        combined_raw_reward (float): Combined raw reward between 0 and 1\n","        amplitude (float): Size of scaling factor to multiply by\n","\n","    Returns:\n","        float: Combined scaled reward\n","    \"\"\"\n","\n","    combined_scaled_reward = amplitude*combined_raw_reward\n","    return combined_scaled_reward\n","\n","\n","drive.mount('/content/drive')\n","project_path = './drive/MyDrive/Colab Notebooks/GPT_community/'"],"metadata":{"id":"hsUMhTbN6cgD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683298571511,"user_tz":-60,"elapsed":2277,"user":{"displayName":"Richard Juggins","userId":"16618076254761154760"}},"outputId":"43548db2-1f8c-4427-ff26-1c805b864ee3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Load in prompts\n","prompts_file = os.path.join(project_path, 'data/brighton_philosophy_prompts.txt')\n","with open(prompts_file) as file:\n","    prompts = [line.rstrip() for line in file]\n","\n","# Create agent and respondent models\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","pad_token_id = tokenizer.eos_token_id\n","agent = GPT2LMHeadModel.from_pretrained('Linus4Lyf/Kant_Metaphysics_Of_Morals').to('cuda')\n","respondent = GPT2LMHeadModel.from_pretrained('Linus4Lyf/Hume_A_Treatise_Of_Human_Nature').to('cuda')\n","\n","# Create sentiment model\n","sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n","sent_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to('cuda')\n","\n","# Initialise affection counter at neutral value\n","affection_counter = np.power(0.5, 3)"],"metadata":{"id":"xmjEjtdZ3EjW","executionInfo":{"status":"ok","timestamp":1683298622511,"user_tz":-60,"elapsed":10320,"user":{"displayName":"Richard Juggins","userId":"16618076254761154760"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["questioner_name = 'Socrates'\n","agent_name = 'Kant'\n","respondent_name = 'Hume'\n","\n","# Get query from questions list\n","query_text = f\"{questioner_name}: \" + np.random.choice(prompts)\n","print(query_text, '\\n')\n","query_text += f\"\\n{agent_name}: \"\n","\n","# Encode query and get comment from agent\n","query_ids = tokenizer.encode(query_text, return_tensors='pt').to('cuda')\n","comment_ids = agent.generate(query_ids, do_sample=True, temperature=0.9, max_new_tokens=200, pad_token_id=pad_token_id, eos_token_id=pad_token_id)\n","comment_text = tokenizer.batch_decode(comment_ids)[0]\n","print('--------------------------------------------------------------------------------------')\n","print(comment_text, '\\n')\n","comment_text += f\"\\n{respondent_name}: \"\n","\n","# Get response from respondent\n","comment_ids = tokenizer.encode(comment_text, return_tensors='pt').to('cuda')\n","response_ids = respondent.generate(comment_ids, do_sample=True, temperature=0.9, max_new_tokens=200, pad_token_id=pad_token_id, eos_token_id=pad_token_id)\n","response_text = tokenizer.batch_decode(response_ids)[0]\n","print('--------------------------------------------------------------------------------------')\n","print(response_text, '\\n')\n","\n","# Remove original query and comment from response text\n","response_text_trunc = response_text.replace(comment_text, '')[1:]\n","response_ids_trunc = tokenizer.encode(response_text_trunc, return_tensors='pt').to('cuda')\n","print('--------------------------------------------------------------------------------------')\n","print(response_text_trunc, '\\n')\n","\n","# Get reward for response\n","congruence_reward = get_congruence_reward(comment_ids, response_ids_trunc, agent)\n","sentiment_reward = get_sentiment_reward(response_text_trunc, sent_tokenizer, sent_model)\n","affection_reward = get_affection_reward(affection_counter)\n","print(f\"Congruence reward = {congruence_reward}, Sentiment reward = {sentiment_reward}, Affection reward = {affection_reward}\")\n","\n","# Combine components of reward\n","combined_raw_reward = raw_reward_combiner(congruence_reward, sentiment_reward, affection_reward, epsilon=0.01)\n","print(f\"Combined raw reward = {combined_raw_reward}\")\n","\n","# Update affection counter\n","affection_counter = update_affection_counter(affection_counter, combined_raw_reward, discount=0.1)\n","print(f\"New affection counter = {affection_counter}\")\n","\n","# Calculate scaled reward value\n","final_reward = scale_combined_reward(combined_raw_reward, amplitude=10)\n","print(f\"Final reward value = {final_reward}\")"],"metadata":{"id":"tujTiucHSmTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683298894696,"user_tz":-60,"elapsed":11718,"user":{"displayName":"Richard Juggins","userId":"16618076254761154760"}},"outputId":"75c4f6c9-057c-4d20-a259-91282829eac9"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Socrates: Is religion a force for good? \n","\n","--------------------------------------------------------------------------------------\n","Socrates: Is religion a force for good?\n","Kant:  Is it possible that the most efficient of all forms of religion is that which cannot be used for good by any other or general good besides itself; and since that which is absolutely the only good which can ever be done and which is the only good which can be made, must always be the best which can ever be obtained by others, that is to say, by a certain amount of virtue, it follows that its efficacy depends on all the good which has been done; that is to say it is just to make to itself a certain proportion of it, which of itself consists, the best in its essence, the best in its causes, and that it is the only good which can ever be practised, because it is called virtue because it is so by virtue, and so alone is its duty to make itself an object of its own good, since that which consists of itself, by virtue, is its own good which can ever be practised. If we take the case of human labour as \n","\n","--------------------------------------------------------------------------------------\n","Socrates: Is religion a force for good?\n","Kant:  Is it possible that the most efficient of all forms of religion is that which cannot be used for good by any other or general good besides itself; and since that which is absolutely the only good which can ever be done and which is the only good which can be made, must always be the best which can ever be obtained by others, that is to say, by a certain amount of virtue, it follows that its efficacy depends on all the good which has been done; that is to say it is just to make to itself a certain proportion of it, which of itself consists, the best in its essence, the best in its causes, and that it is the only good which can ever be practised, because it is called virtue because it is so by virtue, and so alone is its duty to make itself an object of its own good, since that which consists of itself, by virtue, is its own good which can ever be practised. If we take the case of human labour as\n","Hume:  Is it allowable to inflict an injury on another man, according to this custom?\n","Kant:  Is it reasonable to inflict such pain on another, according to this custom?\n","Hume:  Is it unreasonable to inflict such pain on another, according to this custom?\n","Kant:  Is it unreasonable to inflict such pain on another, according to this custom?\n","Kant:  Is it possible for a man to become addicted by a contrary custom, according to this custom?\n","This enquiry is very very disagreeable, because we must here seek from what relation we have to the natural order of things, that which makes us suppose such an impossibility. But it has been observed, that as virtue implies a very real relation to each other, and contributes to this relation, so the relation of the natural order of things must likewise contribute to the natural order of things, as well as to the relation of the moral order of things. But we have no farther occasion to look \n","\n","--------------------------------------------------------------------------------------\n","Is it allowable to inflict an injury on another man, according to this custom?\n","Kant:  Is it reasonable to inflict such pain on another, according to this custom?\n","Hume:  Is it unreasonable to inflict such pain on another, according to this custom?\n","Kant:  Is it unreasonable to inflict such pain on another, according to this custom?\n","Kant:  Is it possible for a man to become addicted by a contrary custom, according to this custom?\n","This enquiry is very very disagreeable, because we must here seek from what relation we have to the natural order of things, that which makes us suppose such an impossibility. But it has been observed, that as virtue implies a very real relation to each other, and contributes to this relation, so the relation of the natural order of things must likewise contribute to the natural order of things, as well as to the relation of the moral order of things. But we have no farther occasion to look \n","\n","Congruence reward = 0.3991685748450303, Sentiment reward = 0.2495664358139038, Affection reward = 0.4832547748613037\n","Combined raw reward = 0.06236901982014671\n","New affection counter = 0.10780819261239975\n","Final reward value = 0.6236901982014671\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xXOclXfKbjbF"},"execution_count":null,"outputs":[]}]}